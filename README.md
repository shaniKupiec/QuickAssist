# QuickAssist: Intent-Aware Chatbot for Customer Support

## ğŸ§  Overview

**QuickAssist** is an NLP course project focused on improving chatbot responses in customer support scenarios by conditioning on *explicit intent labels*. We investigate whether models generate more *accurate*, *helpful*, and *appropriate* responses when given both the customer query and its intent.

> â“ â€œWhy was I charged again?â€
> A chatbot that understands this is a *billing issue* may reply more appropriately than one that doesn't.

---

## ğŸ” Graphical Abstract

![Graphical Abstract](https://github.com/user-attachments/assets/0d0d9f3c-bef1-4430-939f-4c65823a5468)

---

## ğŸ¯ Project Goals

1. **Intent Conditioning**: Evaluate whether providing intent labels improves chatbot response quality.
2. **Pipeline Comparison**: Compare single-step (no intent) vs. two-step (intent-aware) models.
3. **Model Performance**: Test pretrained vs. fine-tuned T5/BERT models for both tasks.
4. **Dataset Generalization**: Compare model performance across two real-world datasets.
5. **Evaluation**: Use both automatic (BLEU, ROUGE, BERTScore) and LLM-based human-like evaluation (helpfulness, fluency, appropriateness).

---

## ğŸ“ Datasets

We use two datasets with pre-annotated intents:

### ğŸ“Œ Bitext

* **Size**: 26,872 QA pairs
* **Intents**: 27, grouped into 11 categories
* **Avg query length**: \~8.69 words
* **Most common intents**: `contact_customer_service`, `complaint`, `check_invoice`

### ğŸ“Œ Customer-Service-for-LLM

* **Size**: 2,700 QA pairs
* **Same 27 intents & 11 categories**
* **Avg query length**: \~8.6 words
* **Most common intents**: `check_invoice`, `switch_account`, `edit_account`

### ğŸ”¹ Example Format

```
Input: instruction (user query): "I want to cancel my subscription"  
Output: response (agent reply): "Sure, I can help with that. Please provide your account ID."  
Intent: intent (pre-annotated intent label): cancel_service  
```

---

## ğŸ§  Model Architecture & Configurations

We implemented both **single-step** and **two-step** architectures to evaluate the effect of intent conditioning on response quality.

### ğŸ§© Two-Step Pipeline

Intent detection and response generation are decoupled:

#### ğŸ”¹ Intent Detection

* **Fine-tuned BERT** (`bert-finetuned`)
  Trained to classify the intent of a customer query.
* **Pretrained T5-base** (`t5-base`)
  Also tested as a zero-shot intent classifier (no fine-tuning).

#### ğŸ”¹ Response Generation

* **Pretrained T5-base** (`t5-base`)
  Used as-is to generate responses based on query + intent.
* **Fine-tuned T5** (`t5-finetuned`)
  Trained on our labeled data to generate more accurate, context-aware responses.

> âœ… Best-performing configuration: `two_step_complete_ft`
> Uses **fine-tuned BERT** for intent detection and **fine-tuned T5** for response generation.

---

### ğŸ§© Single-Step Pipeline

No explicit intent classification â€” the model generates responses directly from the query.

* **Pretrained T5-base** (`single_step_pretrained`)
  Zero-shot generation, no task-specific tuning.
* **Fine-tuned T5** (`single_step_ft`)
  Trained to map raw customer queries directly to responses.

---

### ğŸ”§ Experiment Configurations

| Name                     | Intent Model   | Response Model | Intent Used? | Fine-tuned?      |
| ------------------------ | -------------- | -------------- | ------------ | ---------------- |
| `single_step_pretrained` | â€”              | T5-base        | âŒ            | No               |
| `single_step_ft`         | â€”              | T5-finetuned   | âŒ            | Yes (T5)         |
| `two_step_baseline`      | Ground-truth   | T5-base        | âœ…            | No               |
| `two_step_pretrained`    | T5-base        | T5-base        | âœ…            | No               |
| `two_step_partial_ft`    | BERT-finetuned | T5-base        | âœ…            | Partial (BERT)   |
| `two_step_complete_ft`   | BERT-finetuned | T5-finetuned   | âœ…            | Full (BERT + T5) |

---

## ğŸ§ª Evaluation

We used a combination of automatic, human-like, and intent-level metrics:

* **Automatic**: BERTScore, ROUGE (1/2/L), BLEU
* **Human-like (LLM-based)**: Helpfulness, Fluency, Appropriateness
* **Intent Accuracy**: Calculated for two-step models where the intent is predicted by the model

> âœ” Evaluation results are saved to: `metrics.json`, `intent_accuracy.json`, `human_scores.csv`

---

## ğŸ“ˆ Key Results

| Configuration            | BERTScore (F1) | BLEU | ROUGE-L |
| ------------------------ | -------------- | ---- | ------- |
| `single_step_pretrained` | Baseline       | â†“    | â†“       |
| `two_step_complete_ft`   | âœ… Best         | â†‘    | â†‘       |

âœ” Intent conditioning improves chatbot performance
âœ” Fine-tuning significantly boosts BLEU and BERTScore
âœ” Two-step (intent-aware) models consistently outperform single-step models

---

## ğŸ“‚ Repository Structure

This repository includes all three project stages with both **presentations** and **code**:

```bash
QuickAssist/
â”œâ”€â”€ Project_Proposal_Presentation.pdf      # Initial idea and proposal
â”œâ”€â”€ Mid_PPT/
â”‚   â”œâ”€â”€ requirements.txt                   # Mid-stage dependencies
â”‚   â”œâ”€â”€ Mid_Project_Presentation.pdf       # Mid-project slides
â”‚   â””â”€â”€ README_Mid_PPT.md                  # Mid-stage documentation
â”œâ”€â”€ Final_PPT/
â”‚   â”œâ”€â”€ requirements.txt                   # Final-stage dependencies
â”‚   â”œâ”€â”€ Final_Project_Presentation.pdf     # Final slides
â”‚   â””â”€â”€ README_Final_PPT.md                # Detailed model, results & evaluation
```

---

## ğŸ§ª Running the Code

To run our code, see the instructions in:

* [Mid-PPT README](https://github.com/shaniKupiec/QuickAssist/blob/main/Mid_PPT/README_Mid_PPT.md)
* [Final-PPT README](https://github.com/shaniKupiec/QuickAssist/blob/main/Final_PPT/README_Final_PPT.md)

---

## ğŸ‘¥ Team

* [Inbal Bolshinsky](https://github.com/InbalBolshinsky)
* [Shani Kupiec](https://github.com/shaniKupiec)
* [Almog Sasson](https://github.com/Almog-Sasson)
* [Nadav Margaliot](https://github.com/NadavMargaliot)

ğŸ“ NLP Course, Holon Institute of Technology (HIT)

---

Would you like me to export this as a `README.md` file for GitHub upload?
